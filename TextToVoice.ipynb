{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOGR5wz2ifhzxS5pxjttWO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashgoel/tts/blob/main/TextToVoice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html"
      ],
      "metadata": {
        "id": "vfwhCKn1sl0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install deep-phonemizer"
      ],
      "metadata": {
        "id": "psmutOSirub5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ1noyCzraSn",
        "outputId": "00743251-49a4-461d-e970-3e77acde5cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "2.0.2+cu118\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
        "\n",
        "processor = bundle.get_text_processor()\n",
        "tacotron2 = bundle.get_tacotron2().to(device)\n",
        "vocoder = bundle.get_vocoder().to(device)\n",
        "\n",
        "text = \"\"\"\n",
        "Friends have been urging me to write to you for the sake of humanity.\n",
        "But I have resisted their request, because of the feeling that any letter from me would be an impertinence.\n",
        "Something tells me that I must not calculate and that I must make my appeal for whatever it may be worth.\n",
        "It is quite clear that you are today the one person in the world who can prevent a war which may reduce humanity to the savage state.\n",
        "Must you pay that price for an object however worthy it may appear to you to be?\n",
        "Will you listen to the appeal of one who has deliberately shunned the method of war not without considerable success?\n",
        "Any way I anticipate your forgiveness, if I have erred in writing to you\n",
        "\"\"\"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    processed, lengths = processor(text)\n",
        "    processed = processed.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
        "    waveforms, lengths = vocoder(spec, spec_lengths)\n",
        "\n",
        "# fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))\n",
        "# ax1.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
        "# ax2.plot(waveforms[0].cpu().detach())\n",
        "\n",
        "IPython.display.Audio(waveforms[0:1].cpu(), rate=vocoder.sample_rate)"
      ],
      "metadata": {
        "id": "ZQOCBdHxsy_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wavenet\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from wavenet_vocoder import WaveNet\n",
        "\n",
        "# Load the pre-trained WaveNet model\n",
        "wavenet = WaveNet().to('cuda')  # Make sure to use GPU if available\n",
        "checkpoint = torch.load('wavenet_model.pth')\n",
        "wavenet.load_state_dict(checkpoint['state_dict'])\n",
        "wavenet.eval()\n",
        "\n",
        "# Load and preprocess the input mel-spectrogram\n",
        "mel_spectrogram, sample_rate = torchaudio.load('input_mel_spectrogram.pt')  # Replace with your own input\n",
        "mel_spectrogram = mel_spectrogram.to('cuda')  # Move to GPU if available\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    # Pass the mel-spectrogram through the WaveNet model\n",
        "    audio = wavenet.infer(mel_spectrogram)\n",
        "\n",
        "# Save the synthesized audio\n",
        "torchaudio.save('synthesized_audio.wav', audio.cpu(), sample_rate)  # Replace with your desired output path\n",
        "\n",
        "print(\"Synthesized audio saved successfully!\")\n"
      ],
      "metadata": {
        "id": "kjrhrpC6xqb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from wavenet_vocoder import WaveNet\n",
        "from melgan_vocoder import MelGAN\n",
        "\n",
        "# Define the list of vocoder models\n",
        "vocoder_models = [\n",
        "    {\n",
        "        'name': 'WaveNet',\n",
        "        'model': WaveNet().to('cuda'),\n",
        "        'checkpoint': torch.load('wavenet_model.pth')\n",
        "    },\n",
        "    {\n",
        "        'name': 'MelGAN',\n",
        "        'model': MelGAN().to('cuda'),\n",
        "        'checkpoint': torch.load('melgan_model.pth')\n",
        "    },\n",
        "    # Add more vocoder models as needed\n",
        "]\n",
        "\n",
        "# Load and preprocess the input mel-spectrogram\n",
        "mel_spectrogram, sample_rate = torchaudio.load('input_mel_spectrogram.pt')  # Replace with your own input\n",
        "mel_spectrogram = mel_spectrogram.to('cuda')  # Move to GPU if available\n",
        "\n",
        "# Perform inference and measure performance for each vocoder model\n",
        "for vocoder in vocoder_models:\n",
        "    vocoder_model = vocoder['model']\n",
        "    vocoder_checkpoint = vocoder['checkpoint']\n",
        "\n",
        "    vocoder_model.load_state_dict(vocoder_checkpoint['state_dict'])\n",
        "    vocoder_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = torch.cuda.Event(enable_timing=True)\n",
        "        end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start_time.record()\n",
        "        audio = vocoder_model.infer(mel_spectrogram)\n",
        "        end_time.record()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        inference_time = start_time.elapsed_time(end_time)\n",
        "\n",
        "    output_path = f\"synthesized_audio_{vocoder['name'].lower()}.wav\"\n",
        "    torchaudio.save(output_path, audio.cpu(), sample_rate)\n",
        "\n",
        "    print(f\"{vocoder['name']} inference time:\", inference_time, \"ms\")\n",
        "\n",
        "print(\"Synthesized audios saved successfully!\")\n"
      ],
      "metadata": {
        "id": "FGXOTUkByO8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}